{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8676a892-dd21-4801-a14b-f2f95b3070ae",
   "metadata": {},
   "source": [
    "# Build-a-PHG\n",
    "This notebook interactively walks through [Steps 1 and 2](https://bitbucket.org/bucklerlab/practicalhaplotypegraph/wiki/UserInstructions/CreatePHG_step1-2_main) of the PHG Wiki's user setup instructions.\n",
    "\n",
    "The steps are as follows:\n",
    "\n",
    "1. [Generate GVCFs from Assembly MAFs](#Step-1:-Generate-GVCFs-from-Assembly-MAFs)\n",
    "2. [Generate Wiggles from MAFs](#Step-2:-Generate-Wiggles-from-MAFs)\n",
    "3. [Generate and Validate Reference Range BED File](#Step-3:-Generate-and-Validate-Reference-Range-BED-File)\n",
    "4. [Create Database and Load Haplotypes Into PHG](#Step-4:-Create-Database-and-Load-Haplotypes-Into-PHG)\n",
    "5. [Create Consensus Haplotypes](#Step-5:-Create-Consensus-Haplotypes)\n",
    "6. [Impute Variants with the PHG](#Step-6:-Impute-Variants-with-the-PHG)\n",
    "\n",
    "Review the documentation to ensure the command-line arguments are correct for your data. Some of the code here is specific to the needs of the NextGen Cassava project, and so may not be relevant to your data.\n",
    "\n",
    "### Assumptions\n",
    "* You have already selected a reference and its associated FASTA and GFF.\n",
    "* You'll be loading haplotypes into the database from MAF files generated by [AnchorWave](https://github.com/baoxingsong/AnchorWave).\n",
    "\n",
    "Additionally, this notebook assumes your \"working directory\" – where your input files are located, and where the PHG's data will reside – will be relative to where this notebook is stored.\n",
    "\n",
    "### Requirements\n",
    "Ensure the following software is available to be executed (in your `PATH`) before proceeding:\n",
    "\n",
    "* `docker`: https://docs.docker.com/get-docker/\n",
    "* `faSize`: https://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/\n",
    "* `kotlinc`: https://github.com/JetBrains/kotlin/releases/download/v1.7.21/kotlin-compiler-1.7.21.zip\n",
    "    * After unzipping, the binary is in the `bin/` directory - only for x86-64\n",
    "\n",
    "### Configuration\n",
    "Before we begin, we must import packages and initialize variables which we will be using later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b244d686-7efe-4a92-b61d-bec86050d044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from sys import platform\n",
    "\n",
    "mkdirp = lambda d: os.makedirs(d, exist_ok = True)\n",
    "\n",
    "mkdirp(\"workdir\")\n",
    "WORKING_DIR = f\"{os.getcwd()}/workdir\"\n",
    "\n",
    "PHG_DIR = f\"{WORKING_DIR}/phg\"\n",
    "mkdirp(PHG_DIR)\n",
    "\n",
    "mkdirp(f\"{PHG_DIR}/local_gvcf\")\n",
    "LOCAL_GVCF = \"/phg/local_gvcf\"\n",
    "\n",
    "mkdirp(f\"{PHG_DIR}/inputDir/reference\")\n",
    "\n",
    "PGDATA_DIR = f\"{WORKING_DIR}/pgdata\"\n",
    "mkdirp(PGDATA_DIR)\n",
    "\n",
    "HOST_IP = ! ifconfig | grep \"inet \" | grep -Fv 127.0.0.1 | awk '{print $2}'\n",
    "HOST_IP = HOST_IP[0]\n",
    "\n",
    "USER_ID = ! whoami | id -u && whoami | id -g\n",
    "USER_ID = \":\".join(USER_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2809d9fa-52a4-4657-96fb-36ffc8623ec2",
   "metadata": {},
   "source": [
    "Please modify the below variables to refer to the files you will be using, relative to the directory this notebook is being run from. You may also want to change the Docker-related variables if your directory structure is different from the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92025d1-c071-48f4-ad8f-5d22f58a3bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# EDIT ME! #\n",
    "############\n",
    "REF = \"Mesculenta_671_v8.0.fa\"\n",
    "GFF = \"Mesculenta_671_v8.1.gene.gff3\"\n",
    "\n",
    "DOCKER_REF = f\"/phg/inputDir/reference/{REF}\"\n",
    "DOCKER_GFF = f\"/phg/{GFF}\"\n",
    "DOCKER_CONFIG = \"/phg/config.txt\"\n",
    "\n",
    "# Change based upon the memory resources you wish to allocate from your machine\n",
    "# g for gigabytes, m for megabytes, or k for kilobytes\n",
    "JAVA_XMS = \"-Xms32g\"\n",
    "JAVA_XMX = \"-Xmx64g\"\n",
    "\n",
    "# External storage of gvcf and reference\n",
    "SERVER_PATH_ROOT = \"example.com;/some/directory\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bced65e6-d453-44ff-a5fc-a4ae146c5141",
   "metadata": {},
   "source": [
    "Create the PHG directory structure and copy our reference's FASTA and GFF to their default locations.\n",
    "\n",
    "Notice the `--user` argument: it's necessary to ensure files and directories are not owned by root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20281a25-9809-4a82-b635-3947febf95a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker run --name create_phg_dirs --rm \\\n",
    "    -v {PHG_DIR}/:/phg/ \\\n",
    "    --user {USER_ID} \\\n",
    "    -t maizegenetics/phg:latest \\\n",
    "    /tassel-5-standalone/run_pipeline.pl -debug \\\n",
    "    -MakeDefaultDirectoryPlugin \\\n",
    "        -workingDir /phg/ \\\n",
    "    -endPlugin\n",
    "\n",
    "! cp {REF} {WORKING_DIR}{DOCKER_REF}\n",
    "! cp {GFF} {WORKING_DIR}{DOCKER_GFF}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e876eb8a-6fbc-496f-8c71-347bbc9c8775",
   "metadata": {},
   "source": [
    "## Step 1: Generate GVCFs from Assembly MAFs\n",
    "Here we execute the `MAFToGVCFPlugin` in the PHG via a Docker container to generate [GVCF](https://gatk.broadinstitute.org/hc/en-us/articles/360035531812-GVCF-Genomic-Variant-Call-Format) files from the MAF files produced by AnchorWave. Create the `mafs/` directory in the same location as this notebook and transfer your MAFs to it.\n",
    "\n",
    "Note that depending on your input MAF, you may need to change the `-twoGvcfs` option to `false`. In our case it is a diploid alignment, so we set the option to `true` because we want two separate GVCF files. See [this section](https://bitbucket.org/bucklerlab/practicalhaplotypegraph/wiki/UserInstructions/CreatePHG_step2_MAFToGVCFPluginDetails#markdown-header-parameter-descriptions) of the wiki for a full explanation of the parameters for this plugin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1343d38-9fce-41f0-a486-34fbc3806ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"mafs\"):\n",
    "    raise FileNotFoundError(\"Please create and populate the mafs/ directory with your files\")\n",
    "mafs = glob.glob(\"mafs/**/*.maf\", recursive = True)\n",
    "\n",
    "mkdirp(f\"{PHG_DIR}/mafs\")\n",
    "\n",
    "for maf in mafs:\n",
    "    ! cp {maf} {PHG_DIR}/mafs\n",
    "    file_name = maf.split(\"/\")[-1]\n",
    "    name = file_name.split(\".\")[0]\n",
    "    gvcf_output = f\"/phg/inputDir/loadDB/gvcf/{name}.gvcf.gz\"\n",
    "    \n",
    "    ! docker run --name maf_to_gvcf_{name} --rm \\\n",
    "        -v {PHG_DIR}/:/phg/ \\\n",
    "        --user {USER_ID} \\\n",
    "        -t maizegenetics/phg:latest \\\n",
    "        /tassel-5-standalone/run_pipeline.pl {JAVA_XMS} {JAVA_XMX} -debug \\\n",
    "        -MAFToGVCFPlugin \\\n",
    "            -referenceFasta {DOCKER_REF} \\\n",
    "            -mafFile /phg/mafs/{file_name} \\\n",
    "            -sampleName {name} \\\n",
    "            -gvcfOutput {gvcf_output} \\\n",
    "            -fillGaps false -twoGvcfs true \\\n",
    "        -endPlugin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f7f22f-64d3-409b-b6fc-dfc0debd4d10",
   "metadata": {},
   "source": [
    "After the GVCFs are created, we must produce a [keyfile](https://bitbucket.org/bucklerlab/practicalhaplotypegraph/wiki/UserInstructions/CreatePHG_step2_LoadHaplotypesFromGVCFPluginDetails.md#markdown-header-keyfile) from them for use by the PHG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978885de-60a6-4118-bab6-23a9aa8e7cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADER_COL = \"type\\tsample_name\\tsample_description\\tmafFile\\tfiles\\tchrPhased\\tgenePhased\\tphasingConf\\tgvcfServerPath\\n\"\n",
    "gvcf_paths = glob.glob(f\"{PHG_DIR}/inputDir/loadDB/gvcf/*.gvcf.gz\")\n",
    "\n",
    "cultivars = dict()\n",
    "for gvcf_path in gvcf_paths:\n",
    "    gvcf_filename = gvcf_path.split(\"/\")[-1]\n",
    "    sample_name = gvcf_filename.split(\".gvcf.gz\")[0]\n",
    "    if sample_name not in cultivars:\n",
    "        cultivars[sample_name] = []\n",
    "    cultivars[sample_name].append(gvcf_filename)\n",
    "\n",
    "# NOTE:\n",
    "#  - What values should be used for `chrPhased`, `genePhased`, and `phasingConf`?\n",
    "with open(f\"{PHG_DIR}/gvcfKeyFile.txt\", \"w\") as kf:\n",
    "    kf.write(HEADER_COL)\n",
    "    for cultivar in cultivars.keys():\n",
    "        entry = f\"GVCF\\t{cultivar}_Assembly\\t{cultivar} description\\t/phg/outputDir/align/{cultivar}.maf\\t{','.join(cultivars[cultivar])}\\ttrue\\ttrue\\t0.9\\t{SERVER_PATH_ROOT}\\n\"\n",
    "        kf.write(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec1a4a2-2f7f-4046-bfee-eb96aed1b7f3",
   "metadata": {},
   "source": [
    "## Step 2: Generate Wiggles from MAFs\n",
    "The MAFs are also used to create [Wiggle](https://genome.ucsc.edu/goldenPath/help/wiggle.html) files. These, alongside the GVCFs, are used in *Step 3* to generate the reference range intervals (or \"anchors\").\n",
    "\n",
    "Here we utilize `faSize` to output the name and size of each record within the reference FASTA. Afterwards, we run [this Kotlin script](https://bitbucket.org/bucklerlab/biokotlin/src/master/src/main/kotlin/biokotlin/genome/wiggle_fromMAFmultiThread.kts) to generate the wiggles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ce26d8-bf4b-4334-9cce-97c81f77706e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdirp(f\"{PHG_DIR}/wiggles\")\n",
    "\n",
    "! faSize -detailed {REF} > mesculenta_fasize.txt\n",
    "\n",
    "! grep \"^Chromosome\" mesculenta_fasize.txt | sort > mesculenta_fasize_sorted.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510c0d9b-23b2-48cd-bb7c-cafd644c8ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Wiggle script, add dependency annotation\n",
    "! wget -O wiggle_fromMAFmultiThread.main.kts https://bitbucket.org/bucklerlab/biokotlin/raw/6b5379534d1e1988039a0decd47fbdef3f878f91/src/main/kotlin/biokotlin/genome/wiggle_fromMAFmultiThread.kts\n",
    "\n",
    "# Change usage of sed if on MacOS\n",
    "# Source: https://stackoverflow.com/a/21243111\n",
    "if platform == \"darwin\":\n",
    "    ! sed -i '' '1s/^/@file:DependsOn(\"org\\.biokotlin:biokotlin:0\\.05\\.01\")\\n\\n/' wiggle_fromMAFmultiThread.main.kts\n",
    "else:\n",
    "    ! sed -i '1s/^/@file:DependsOn(\"org\\.biokotlin:biokotlin:0\\.05\\.01\")\\n\\n/' wiggle_fromMAFmultiThread.main.kts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1ccf28-5b6c-44bf-a87e-4fdde4d34ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mesculenta_fasize_sorted.txt\", \"r\") as mfa:\n",
    "    chroms = mfa.readlines()\n",
    "    for chrom in chroms:\n",
    "        contig_name, end_pos = chrom.split(\"\\t\")\n",
    "        contig_name = contig_name.strip()\n",
    "        end_pos = end_pos.strip()\n",
    "        print(f\"Creating {contig_name} wiggle\")\n",
    "        ! _JAVA_OPTIONS={JAVA_XMX} kotlinc -script wiggle_fromMAFmultiThread.main.kts -- -mafDir {PHG_DIR}/mafs/ -mafContig {contig_name} -wiggleContig {contig_name} -start 1 -end {end_pos} -outputDir {PHG_DIR}/wiggles/\n",
    "\n",
    "! rm {PHG_DIR}/wiggles/identity_*.wig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d42f27-500f-462b-a5e7-9bac357a1ae4",
   "metadata": {},
   "source": [
    "## Step 3: Generate and Validate Reference Range BED File\n",
    "**TODO:** Add explanation about tweaking parameters to `CreateRefRangesPlugin` in order to produce ranges that are biologically sound: `minCover`, `windowSize`, `intergenicStepSize`, `maxSearchWindow`, `maxDiversity`, `minGenicLength`, and `maxClusters`.\n",
    "\n",
    "Use the artifacts of *Step 1* and *Step 2* to create a [BED](https://genome.ucsc.edu/FAQ/FAQformat.html#format1) file. You can break the reference genome into any set of intervals you want.\n",
    "\n",
    "Ensure that `minCover` is less than or equal to the number of taxa, otherwise a cut site will not be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890b0674-ccc2-4795-9ce3-890c79e67fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker run --name create_ref_ranges --rm \\\n",
    "    -v {PHG_DIR}/:/phg \\\n",
    "    --user {USER_ID} \\\n",
    "    -t maizegenetics/phg:latest \\\n",
    "    /tassel-5-standalone/run_pipeline.pl {JAVA_XMS} {JAVA_XMX} -debug \\\n",
    "    -CreateRefRangesPlugin \\\n",
    "        -wiggleDir /phg/wiggles/ \\\n",
    "        -gffFile {DOCKER_GFF} \\\n",
    "        -minCover 2 \\\n",
    "        -outputBedFile /phg/refRanges.bed \\\n",
    "        -refGenome {DOCKER_REF} \\\n",
    "        -vcfdir /phg/inputDir/loadDB/gvcf \\\n",
    "        -outputGeneRanges /phg/geneRanges.bed \\\n",
    "        -nThreads 22 \\\n",
    "    -endPlugin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee5a6e9-6ddc-4098-8dbb-c4894f7fa635",
   "metadata": {},
   "source": [
    "Ensure your BED file contains no overlaps - the `CreateValidIntervalsFilePlugin` does this for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d408c1-88d1-4624-ac72-a18d9d671691",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker run --name validate_ref_ranges --rm \\\n",
    "    -v {PHG_DIR}/:/phg \\\n",
    "    --user {USER_ID} \\\n",
    "    -t maizegenetics/phg:latest \\\n",
    "    /tassel-5-standalone/run_pipeline.pl {JAVA_XMS} {JAVA_XMX} -debug \\\n",
    "    -CreateValidIntervalsFilePlugin \\\n",
    "        -intervalsFile /phg/refRanges.bed \\\n",
    "        -referenceFasta {DOCKER_REF} \\\n",
    "        -mergeOverlaps true \\\n",
    "        -generatedFile /phg/validRefRanges.bed \\\n",
    "    -endPlugin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610e6174-c8df-4c3b-850f-ae9dd77108e1",
   "metadata": {},
   "source": [
    "Remove the header from the `validRefRanges.bed` file. The `LoadHaplotypesFromGVCFPlugin` does not expect the header that the `CreateValidIntervalsFilePlugin` adds. This will be fixed in future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a390d61-7cfa-4033-8b8e-b7c8113d50f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tail to get all lines, skipping the first\n",
    "! tail -n +2 {PHG_DIR}/validRefRanges.bed > {PHG_DIR}/refRanges.bed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3808fa95-0180-4d2a-b026-1a08c8e56b2f",
   "metadata": {},
   "source": [
    "## Step 4: Create Database and Load Haplotypes Into PHG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88753fbc-2901-4287-8665-dfab953252f3",
   "metadata": {},
   "source": [
    "Start the PostgreSQL database container. Be aware that although Jupyter will display the cell as completed, the Postgres container may still be initializing - watch the log output of Docker (via the desktop application or `docker logs -f <container_name>` in a terminal) to determine if the database is ready. The server will start and create the database specified by `POSTGRES_DB` before restarting again. Therefore there will be two `LOG:  database system is ready to accept connections` messages, *wait until you see the second before continuing!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d89b08-bf8b-44bb-82f9-d26e0daccc13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NOTE: POSTGRES_HOST_AUTH_METHOD should NOT be `trust` which is insecure\n",
    "#       However, I am still encountering the error below when using `md5` or the default `scram-sha-256`\n",
    "#       `DBLoadingUtils:getPostgresconnection: exception thrown, The authentication type 10 is not supported. Check that you have configured the pg_hba.conf file to include the client's IP address or subnet, and that it is using an authentication scheme supported by the driver.`\n",
    "# NOTE: User must be `postgres` here and in config.txt below, otherwise we encounter the error:\n",
    "#       `Could not get create/retrieve database phg_test_db, error: ERROR: role \"postgres\" does not exist`\n",
    "! docker run --name postgres-phg \\\n",
    "    --user {USER_ID} \\\n",
    "    -e POSTGRES_USER=postgres \\\n",
    "    -e POSTGRES_PASSWORD=phg_test_password \\\n",
    "    -e POSTGRES_DB=phg_test_db \\\n",
    "    -e POSTGRES_HOST_AUTH_METHOD=trust \\\n",
    "    -v {PGDATA_DIR}/:/var/lib/postgresql/data \\\n",
    "    -p 5432:5432 \\\n",
    "    -d postgres:11-bullseye \\\n",
    "    -c password_encryption=md5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e938e47-0ead-47b8-9fd9-cd277642e84e",
   "metadata": {
    "tags": []
   },
   "source": [
    "Create `load_genome_data.txt` which is a tab-delimited file containing information related to the reference genome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b72a1a-0e3f-4d44-83fb-0d33ea09486e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgd_txt = f\"\"\"Genotype\tHapnumber\tDataline\tPloidy\tGenesPhased\tChromsPhased\tConfidence\tMethod\tMethodDetails\tgvcfServerPath\n",
    "{REF}\t0\treference\t1\tfalse\tfalse\t1\tReferenceMethod\tReferenceMethodDetails\t{SERVER_PATH_ROOT}\"\"\"\n",
    "! echo '{lgd_txt}' > {PHG_DIR}/inputDir/reference/load_genome_data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2da74f4-de35-4fd6-a6ad-0bce89668c32",
   "metadata": {},
   "source": [
    "Modify the default `config.txt` in the PHG directory. Some things to keep in mind:\n",
    "* Since we are utilizing WGS data, `haplotypeMethodName` is user assigned. Read more [on the wiki](https://bitbucket.org/bucklerlab/practicalhaplotypegraph/wiki/UserInstructions/HaplotypeMethod).\n",
    "* `BestHaplotypePathPlugin.minTaxa` should be less than or equal to the number of taxa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae3f704-09a4-4b7b-960e-586b622ec020",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_config_file = f\"{PHG_DIR}/config.txt\"\n",
    "\n",
    "config_contents = f\"\"\"host={HOST_IP}:5432\n",
    "user=postgres\n",
    "password=phg_test_password\n",
    "DB=phg_test_db\n",
    "DBtype=postgres\n",
    "\n",
    "referenceFasta=/phg/inputDir/reference/{REF}\n",
    "anchors=/phg/refRanges.bed\n",
    "refServerPath={SERVER_PATH_ROOT}\n",
    "genomeData=/phg/inputDir/reference/load_genome_data.txt\n",
    "localGVCFFolder={LOCAL_GVCF}\n",
    "\n",
    "LoadHaplotypesFromGVCFPlugin.referenceFasta=/phg/inputDir/reference/{REF}\n",
    "LoadHaplotypesFromGVCFPlugin.gvcfDir=/phg/inputDir/loadDB/gvcf\n",
    "LoadHaplotypesFromGVCFPlugin.wgsKeyFile=/phg/gvcfKeyFile.txt\n",
    "LoadHaplotypesFromGVCFPlugin.bedFile=/phg/refRanges.bed\n",
    "LoadHaplotypesFromGVCFPlugin.haplotypeMethodName=assembly_by_anchorwave\n",
    "LoadHaplotypesFromGVCFPlugin.haplotypeMethodDescription=\"files aligned with anchorwave, then turned to gvcf with plugin\"\n",
    "\n",
    "SAMToMappingPlugin.keyFile=/phg/readMapping_key_file.txt\n",
    "SAMToMappingPlugin.samDir=/phg/inputDir/imputation/sam\n",
    "SAMToMappingPlugin.methodDescription=\"Explain how your input SAMs were produced\"\n",
    "\n",
    "BestHaplotypePathPlugin.pathMethod=assembly_by_anchorwave123\n",
    "\n",
    "# TODO: Temporary fix for ImputePipelinePlugin\n",
    "keyFile=/phg/readMapping_key_file\n",
    "\n",
    "pathMethod=assembly_by_anchorwave\n",
    "outVcfFile=/phg/outputDir/imputation-results.vcf\n",
    "\n",
    "# Default is 20, but in our case we have less than that\n",
    "BestHaplotypePathPlugin.minTaxa=2\n",
    "\n",
    "outputDir=/phg/outputDir\n",
    "liquibaseOutdir=/phg/outputDir/\"\"\"\n",
    "\n",
    "! echo '{config_contents}' > {docker_config_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4363f242-bd70-4f64-8a8f-382a7454841d",
   "metadata": {},
   "source": [
    "Execute the `MakeInitialPHGDBPipelinePlugin` to initialize the database with the PHG schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d66c76d-31f6-4bd8-8d12-753944c68255",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker run --name create_initial_db --rm \\\n",
    "    -v {PHG_DIR}/:/phg/ \\\n",
    "    --user {USER_ID} \\\n",
    "    -t maizegenetics/phg:latest \\\n",
    "    /tassel-5-standalone/run_pipeline.pl {JAVA_XMS} {JAVA_XMX} -debug \\\n",
    "    -configParameters {DOCKER_CONFIG} \\\n",
    "    -MakeInitialPHGDBPipelinePlugin -endPlugin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62642664-e7e4-4171-ab7f-759f7e90f395",
   "metadata": {},
   "source": [
    "Execute the `LoadHaplotypesFromGVCFPlugin` to populate the PHG using the files you produced in *Steps 1 - 3*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb5d55a-53a0-4e0a-a356-e8abc7c6ce6c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! docker run --name populate_initial_db --rm \\\n",
    "    -v {PHG_DIR}/:/phg/ \\\n",
    "    --user {USER_ID} \\\n",
    "    -t maizegenetics/phg:latest \\\n",
    "    /tassel-5-standalone/run_pipeline.pl {JAVA_XMS} {JAVA_XMX} -debug \\\n",
    "    -configParameters {DOCKER_CONFIG} \\\n",
    "    -LoadHaplotypesFromGVCFPlugin -endPlugin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b846dd5-ca34-4c47-92f0-5140df5e03b6",
   "metadata": {},
   "source": [
    "## Step 5: Create Consensus Haplotypes\n",
    "Before performing consensus, you must first create a `rankingFile.txt`. This is a tab-delimited file of the form `taxon\tscore`, where higher scores mean we trust the taxon more highly.\n",
    "\n",
    "When clustering assemblies - when we have a cluster of similar haplotypes - we choose the taxon in that group which has the higher ranking score. To break ties, be sure to give each taxon a different score. One simple way to score things is to count the number of haplotypes covered by each taxon in the DB and use that count as the score. Any other arbitrary ranking can be used.\n",
    "\n",
    "Be sure to include a ranking for your reference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584d744e-4534-4d90-8ebf-99b56ae09e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_file_txt =  f\"\"\"GCA_003957995_1_Assembly\t1\n",
    "GCA_003957995_2_Assembly\t2\n",
    "GCA_003957885_1_Assembly\t3\n",
    "GCA_003957885_2_Assembly\t4\n",
    "cassava_tme7_phase0_scaffolded_renamed_alignment_1_Assembly\t5\n",
    "cassava_tme7_phase0_scaffolded_renamed_alignment_2_Assembly\t6\n",
    "Mesculenta_671_v8.0.fa\t7\"\"\"\n",
    "! echo '{ranking_file_txt}' > {PHG_DIR}/rankingFile.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d257d4e8-ffed-4b07-98d5-d31a0a0f9cea",
   "metadata": {},
   "source": [
    "Copy the `.gvcf.gz` and `.gvcf.gz.tbi` files for your taxa into the local GVCF directory - including the reference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c264bfd-8ec1-476a-ae34-0318298d182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp {PHG_DIR}/inputDir/reference/{REF}.gvcf.gz* {PHG_DIR}/local_gvcf/.\n",
    "! cp -r {PHG_DIR}/inputDir/loadDB/gvcf/* {PHG_DIR}/local_gvcf/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dbbf35-96a5-40f8-8c8a-e23470e363c1",
   "metadata": {},
   "source": [
    "Next, we create a haplotype graph and attempt to create consensus haplotypes for each anchor. After the consensus haplotypes are created, they will be added to the database and a graph can be generated from them.\n",
    "\n",
    "This step is *optional when using assemblies* and *required with WGS*. In the case of this tutorial, we will be producing consensus haplotypes since we are utilizing WGS data.\n",
    "\n",
    "We highly recommend tuning the clustering parameters to match the diversity present in the database you are working with.\n",
    "\n",
    "**NOTE:** If running a full graph build, remove the `-debug` flag otherwise output will stall your Jupyter browser tab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd898218-2043-4145-b4c7-3cd6d080f60d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! docker run --name create_consensus --rm \\\n",
    "    -v {PHG_DIR}/:/phg/ \\\n",
    "    --user {USER_ID} \\\n",
    "    -t maizegenetics/phg:latest \\\n",
    "    /tassel-5-standalone/run_pipeline.pl {JAVA_XMS} {JAVA_XMX} -debug \\\n",
    "    -configParameters {DOCKER_CONFIG} \\\n",
    "    -HaplotypeGraphBuilderPlugin \\\n",
    "        -configFile {DOCKER_CONFIG} \\\n",
    "        -includeVariantContexts true \\\n",
    "        -localGVCFFolder {LOCAL_GVCF} \\\n",
    "        -methods assembly_by_anchorwave \\\n",
    "    -endPlugin \\\n",
    "    -RunHapConsensusPipelinePlugin \\\n",
    "        -referenceFasta {DOCKER_REF} \\\n",
    "        -dbConfigFile {DOCKER_CONFIG} \\\n",
    "        -collapseMethod My_Informative_Collapse_Method_Name \\\n",
    "        -collapseMethodDetails My_Informative_Collapse_Method_Details \\\n",
    "        -rankingFile /phg/rankingFile.txt \\\n",
    "        -mxDiv 0.00025 \\\n",
    "        -clusteringMode kmer_assembly \\\n",
    "    -endPlugin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d11169-7588-4602-bc27-d467097c31e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 6: Impute Variants with the PHG\n",
    "You have reached imputation! This step uses stored haplotype graph data to infer genotypes from skim sequence, GBS data, or other variant information. It uses the input fastq or variant files to match new individuals to haplotypes in the database and generates paths through the haplotype graph. Paths are stored in the database paths table once they are found. The path information can be output as either haplotype node IDs from the haplotypes table or exported to a VCF file containing SNPs for the taxa processed.\n",
    "\n",
    "**NOTE:** `pangenomeHaplotypeMethod` and `pathHaplotypeMethod` must be set to a valid haplotype method. In our case the value specified by `LoadHaplotypesFromGVCFPlugin.haplotypeMethodName` in `config.txt` from *Step 4*.\n",
    "\n",
    "### A Note on Rerunning the Pipeline with Different Parameters\n",
    "When reprocessing the same samples with different parameter settings, the method names *must* be changed. If the method names are not changed, then read mappings or paths will already exist for those sample names and will not be overwritten. If read mapping parameters used to create the pangenome change, new `readMethod` and `pathMethod` names must be used. If only path finding parameters change, then only change `pathMethod`. In that case the pipeline will use the existing `readMethod` data to compute new paths.\n",
    "\n",
    "If an existing configuration file is modified with new parameter settings and method names, the new configuration file should be saved under a different name. The configuration files provide a record of how analyses were run.\n",
    "\n",
    "### Perform Imputation\n",
    "Prior to running the imputation pipeline we must generate a `pangenome.fa` file from the graph. To do so we use the `WriteFastaFromGraphPlugin`. It requires a haplotype graph, so we chain the `HaplotypeGraphBuilderPlugin` using the same parameters as were used in *Step 5*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c4f921-f7d3-4b56-acbd-1090d5934f6d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "! docker run --name write_pangenome_fasta --rm \\\n",
    "    -v {PHG_DIR}/:/phg/ \\\n",
    "    --user {USER_ID} \\\n",
    "    -t maizegenetics/phg:latest \\\n",
    "    /tassel-5-standalone/run_pipeline.pl {JAVA_XMS} {JAVA_XMX} -debug \\\n",
    "    -configParameters {DOCKER_CONFIG} \\\n",
    "    -HaplotypeGraphBuilderPlugin \\\n",
    "        -configFile {DOCKER_CONFIG} \\\n",
    "        -includeVariantContexts true \\\n",
    "        -localGVCFFolder {LOCAL_GVCF} \\\n",
    "        -methods assembly_by_anchorwave \\\n",
    "    -endPlugin \\\n",
    "    -WriteFastaFromGraphPlugin \\\n",
    "        -outputFile /phg/pangenome.fa \\\n",
    "    -endPlugin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb064b23-6616-4bcc-9b98-98adf98b08f4",
   "metadata": {},
   "source": [
    "Run Minimap2 to index `pangenome.fa`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b693950-d57f-4f6b-b4dd-70163938ac29",
   "metadata": {},
   "outputs": [],
   "source": [
    "! minimap2 -x sr -t 22 -I 16g -d {PHG_DIR}/pangenome.mmi {PHG_DIR}/pangenome.fa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11a16e8-e851-4c57-9c43-c297d4aa16d5",
   "metadata": {},
   "source": [
    "Afterwards, we call Minimap2 using the pangenome FASTA index alongside the chosen taxa's paired-end FASTQs to produce a SAM file to be used as input during imputation.\n",
    "\n",
    "If your short reads  are a bunch of separate files, concatenate each end's reads together so that you have a single FASTQ file for both R1 and R2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72acbd2c-caf9-4330-8f6c-77740ca4f9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! minimap2 -ax sr -t 22 --secondary=yes -N50 --eqx -I 16g {PHG_DIR}/pangenome.mmi {PHG_DIR}/tempFileDir/BGM-2018_R1.fq {PHG_DIR}/tempFileDir/BGM-2018_R2.fq > {PHG_DIR}/inputDir/imputation/sam/BGM-2018.sam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dc54b1-8af1-40be-8238-114a51458520",
   "metadata": {},
   "source": [
    "Create a tab-delimited `readMapping_key_file.txt` containing data about the input SAMs you will be using for imputation - to be processed by the `SAMToMappingPlugin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed846bdc-7161-49c8-95df-a5fdcba61309",
   "metadata": {},
   "outputs": [],
   "source": [
    "readmapping_kf_txt = f\"\"\"cultivar\tflowcell_lane\tfilename\tPlateID\n",
    "BGM-2018\t1\tBGM-2018.sam\t1\"\"\"\n",
    "! echo '{readmapping_kf_txt}' > {PHG_DIR}/readMapping_key_file.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30adce7d-2b0a-493d-84e2-852dc9235137",
   "metadata": {},
   "source": [
    "Additionally, be aware of the `SAMToMappingPlugin.keyFile` and `SAMToMappingPlugin.samDir` entries added to `config.txt` during *Step 4*. You may need to change these values if you are not following the PHG's default directory schema.\n",
    "\n",
    "Next we execute the `ImputePipelinePlugin` to impute variants given our inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bda3911-5021-4422-9ec0-4beea38df054",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! docker run --name imputation --rm \\\n",
    "    -v {PHG_DIR}/:/phg/ \\\n",
    "    -v {WORKING_DIR}/phg.jar:/tassel-5-standalone/lib/phg.jar \\\n",
    "    --user {USER_ID} \\\n",
    "    -t maizegenetics/phg:latest \\\n",
    "    /tassel-5-standalone/run_pipeline.pl {JAVA_XMS} {JAVA_XMX} -debug \\\n",
    "    -configParameters {DOCKER_CONFIG} \\\n",
    "    -ImputePipelinePlugin \\\n",
    "        -readMethod My_Informative_Read_Method_Name \\\n",
    "        -imputeTarget pathToVCF \\\n",
    "        -inputType sam \\\n",
    "        -pangenomeHaplotypeMethod assembly_by_anchorwave \\\n",
    "        -pathHaplotypeMethod assembly_by_anchorwave \\\n",
    "        -localGVCFFolder {LOCAL_GVCF} \\\n",
    "    -endPlugin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521a10ca-9b01-4265-9a53-a61ed760828e",
   "metadata": {},
   "source": [
    "Read mapping counts and imputed paths will be stored in the PHG database. Variants for all the paths stored for `pathMethod` will be written to `outputDir/imputation-results.vcf`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
